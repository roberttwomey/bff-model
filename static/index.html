<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no"/>
  <title>Jetson LLM Voice Chat (VAD)</title>
  <style>
    :root { color-scheme: dark; }
    body { margin:0; font-family: system-ui,-apple-system,Segoe UI,Roboto,sans-serif; background:#0b0c0f; color:#e7e7ea; }
    .wrap { max-width: 820px; margin: 0 auto; padding: 16px; }
    header { display:flex; gap:8px; align-items:center; margin-bottom:12px; }
    input, select, button {
      background:#14161b; color:#e7e7ea; border:1px solid #2a2e36; border-radius:12px; padding:10px 12px; outline:none;
    }
    button { cursor:pointer; }
    .row { display:flex; gap:8px; align-items:center; flex-wrap:wrap; }
    .row > * { flex: 1 1 auto; }
    .chat { display:flex; flex-direction:column; gap:10px; margin-top:10px; padding-bottom:126px; }
    .bubble {
      position:relative; max-width:78%; padding:12px 14px; border-radius:16px; line-height:1.35;
      box-shadow:0 1px 2px rgba(0,0,0,0.25); white-space:pre-wrap;
    }
    .me  { align-self:flex-end; background:#2a2e36; }
    .bot { align-self:flex-start; background:#1a1d24; }
    .bot.streaming { background:#1a2d24; border:1px solid #0d4; }
    .sys { align-self:center; opacity:0.8; font-size:12px; }
    .speak-btn {
      position:absolute; right:8px; bottom:8px; font-size:14px; opacity:0.8;
      border:1px solid #2a2e36; border-radius:999px; padding:2px 8px; background:#14161b;
    }
    .footer {
      position:fixed; bottom:0; left:0; right:0; background:rgba(11,12,15,0.95); backdrop-filter:blur(6px);
      border-top:1px solid #2a2e36; padding:8px;
    }
    .footer .row { gap:6px; }
    .pill { font-size:12px; opacity:0.9; padding:6px 8px; border-radius:99px; border:1px solid #2a2e36; }
    .vad.on { background:#0d5; color:#031; border-color:#0b4; }
    .vad.off { background:#555; color:#ddd; border-color:#333; }
    .vad.state { font-weight:600; }
    .hint { font-size:12px; opacity:0.8; margin-top:6px; }
    .small { font-size:12px; opacity:0.7; }
    .tts-row { gap:8px; margin-top:8px; align-items:center; }
    .tts-row > * { flex:0 0 auto; }
    label { font-size:12px; opacity:0.9; }
    .slider { width:120px; }
  </style>
</head>
<body>
  <div class="wrap">
    <header>
      <strong>Jetson LLM Voice Chat</strong>
      <span class="pill">VAD</span>
      <span id="vadState" class="pill vad state off">Idle</span>
    </header>

    <div class="row" style="margin-bottom:8px">
      <input id="server" placeholder="Server (e.g., http://192.168.4.23:8000)" />
      <select id="modelSelect">
        <option value="">Loading models...</option>
      </select>
      <label><input type="checkbox" id="streamToggle" checked /> üåä Stream</label>
      <button id="save">Save</button>
    </div>
    <div class="row" style="margin-bottom:8px">
      <input id="systemPrompt" placeholder="System prompt (e.g., You are a helpful assistant...)" style="flex: 1;" />
      <button id="savePrompt">Save Prompt</button>
    </div>
    <div class="small">Tap ‚Äúüé§ Start VAD‚Äù once to grant mic permission. The page will listen, auto-start on voice, and auto-stop on silence (~0.6‚Äì1s).</div>

    <!-- TTS controls -->
    <div class="row tts-row">
      <label><input type="checkbox" id="speakToggle" checked /> üîä Speak replies</label>
      <label>Voice:
        <select id="voiceSelect"></select>
      </label>
      <label>Rate:
        <input id="rate" class="slider" type="range" min="0.6" max="1.4" step="0.05" value="1.0">
        <span id="rateVal" class="small">1.00</span>
      </label>
      <label>Pitch:
        <input id="pitch" class="slider" type="range" min="0.6" max="1.6" step="0.05" value="1.0">
        <span id="pitchVal" class="small">1.00</span>
      </label>
    </div>

    <div id="chat" class="chat"></div>
  </div>

  <div class="footer">
    <div class="wrap">
      <div class="row">
        <button id="toggleVAD" class="pill vad off">üé§ Start VAD</button>
        <input id="text" placeholder="Type message‚Ä¶" />
        <button id="send">Send</button>
      </div>
      <div class="hint" id="hint">Status: not listening.</div>
    </div>
  </div>

<script>
/* ------------------------- UI refs ------------------------- */
const el = (id) => document.getElementById(id);
const chatEl   = el('chat');
const srvEl    = el('server');
const modelSelect = el('modelSelect');
const streamToggle = el('streamToggle');
const systemPromptEl = el('systemPrompt');
const saveBtn  = el('save');
const savePromptBtn = el('savePrompt');
const txtEl    = el('text');
const sendBtn  = el('send');
const hintEl   = el('hint');

const toggleVADBtn = el('toggleVAD');
const vadStatePill = el('vadState');

const speakToggle = el('speakToggle');
const voiceSelect = el('voiceSelect');
const rateSlider  = el('rate');   const rateVal  = el('rateVal');
const pitchSlider = el('pitch');  const pitchVal = el('pitchVal');

const storage = {
  get server(){ return localStorage.getItem('server') || `${location.origin}`; },
  set server(v){ localStorage.setItem('server', v); },
  get model(){ return localStorage.getItem('model') || ''; },
  set model(v){ localStorage.setItem('model', v); },
  get stream(){ return localStorage.getItem('stream') !== 'false'; },
  set stream(v){ localStorage.setItem('stream', String(v)); },
  get systemPrompt(){ return localStorage.getItem('systemPrompt') || 'You are SNAPPER, a robot dog and companion.'; },
  set systemPrompt(v){ localStorage.setItem('systemPrompt', v); },
  get history(){ try{ return JSON.parse(localStorage.getItem('history')||'[]'); } catch{ return []; } },
  set history(v){ localStorage.setItem('history', JSON.stringify(v)); },
  get speak(){ return localStorage.getItem('speak') !== 'false'; },
  set speak(v){ localStorage.setItem('speak', String(v)); },
  get voiceURI(){ return localStorage.getItem('voiceURI') || ''; },
  set voiceURI(v){ localStorage.setItem('voiceURI', v); },
  get rate(){ return parseFloat(localStorage.getItem('rate') || '1.0'); },
  set rate(v){ localStorage.setItem('rate', String(v)); },
  get pitch(){ return parseFloat(localStorage.getItem('pitch') || '1.0'); },
  set pitch(v){ localStorage.setItem('pitch', String(v)); },
};

srvEl.value = storage.server;
systemPromptEl.value = storage.systemPrompt;
streamToggle.checked = storage.stream;

let history = storage.history.length ? storage.history : [
  { role: "system", content: storage.systemPrompt }
];
renderAll();

/* ------------------------- Model loading ------------------------- */
async function loadModels() {
  try {
    const server = srvEl.value.replace(/\/+$/, '');
    const response = await fetch(server + '/models');
    if (!response.ok) throw new Error('Failed to fetch models');
    const data = await response.json();
    
    modelSelect.innerHTML = '';
    data.models.forEach(model => {
      const option = document.createElement('option');
      option.value = model;
      option.textContent = model;
      modelSelect.appendChild(option);
    });
    
    // Set saved model if available
    const savedModel = storage.model;
    if (savedModel && data.models.includes(savedModel)) {
      modelSelect.value = savedModel;
    } else if (data.models.length > 0) {
      modelSelect.value = data.models[0];
      storage.model = data.models[0];
    }
  } catch (error) {
    console.error('Failed to load models:', error);
    modelSelect.innerHTML = '<option value="">Error loading models</option>';
  }
}

// Load models when server changes
srvEl.addEventListener('change', loadModels);
srvEl.addEventListener('input', loadModels);

// Load models on page load
loadModels();

// Save model selection when changed
modelSelect.addEventListener('change', () => {
  storage.model = modelSelect.value;
});

// Save stream setting when changed
streamToggle.addEventListener('change', () => {
  storage.stream = streamToggle.checked;
});

// Auto-save system prompt when changed (with debounce)
let promptSaveTimeout;
systemPromptEl.addEventListener('input', () => {
  clearTimeout(promptSaveTimeout);
  promptSaveTimeout = setTimeout(() => {
    const newPrompt = systemPromptEl.value.trim();
    if (newPrompt) {
      storage.systemPrompt = newPrompt;
    }
  }, 1000); // Save after 1 second of no typing
});

/* ------------------------- TTS helpers ------------------------- */
function loadVoicesIntoSelect() {
  const voices = speechSynthesis.getVoices();
  voiceSelect.innerHTML = '';
  voices.forEach(v => {
    const opt = document.createElement('option');
    opt.value = v.voiceURI;
    opt.textContent = `${v.name} (${v.lang})`;
    voiceSelect.appendChild(opt);
  });
  const saved = storage.voiceURI;
  if (saved && voices.some(v => v.voiceURI === saved)) {
    voiceSelect.value = saved;
  } else {
    // Look for Microsoft Ryan voice first
    const ryanVoice = voices.find(v => v.name.includes('Ryan') && v.lang.includes('en-GB'));
    
    if (ryanVoice) {
      voiceSelect.value = ryanVoice.voiceURI;
      storage.voiceURI = ryanVoice.voiceURI;
    } else if (voices[0]) {
      voiceSelect.value = voices[0].voiceURI;
      storage.voiceURI = voices[0].voiceURI;
    }
  }
}
function currentVoice() {
  const uri = voiceSelect.value;
  return speechSynthesis.getVoices().find(v => v.voiceURI === uri) || null;
}
function stopSpeaking() {
  try {
    speechSynthesis.cancel();
  } catch (e) {}
}

// Progressive TTS state - using the robust streaming pattern
const synth = window.speechSynthesis;
let ttsBuffer = '';
let queued = 0;
let currentProgressiveStream = null;

// Progressive TTS constants
const MIN_CHUNK = 40;   // start speaking once we have ~a short phrase
const MAX_CHUNK = 140;  // hard cap to avoid overlong chunks
const PUNCT_PATTERN = /([.!?‚Ä¶]|[\n\r])(?=\s|$)/g;
const QUEUE_LIMIT = 3;  // keep the synth queue short for low latency

function speak(text) {
  try {
    if (!speakToggle.checked || !text) return;
    const u = new SpeechSynthesisUtterance(text);
    const v = currentVoice();
    if (v) u.voice = v;
    u.rate  = parseFloat(rateSlider.value || '1.0');
    u.pitch = parseFloat(pitchSlider.value || '1.0');
    speechSynthesis.cancel();
    speechSynthesis.speak(u);
  } catch (e) {}
}

function makeUtterance(text, voice) {
  const u = new SpeechSynthesisUtterance(text);
  if (voice) u.voice = voice;
  u.rate = parseFloat(rateSlider.value || '1.0');
  u.pitch = parseFloat(pitchSlider.value || '1.0');
  return u;
}

function enqueue(text) {
  if (!text.trim()) return;
  
  const voice = currentVoice();
  const u = makeUtterance(text, voice);
  queued++;
  u.onend = () => queued--;
  u.onerror = () => queued--;
  synth.speak(u);
  
  console.log('TTS enqueued:', text, 'queue size:', queued);
}

function maybeFlush(force = false) {
  if (!ttsBuffer.trim()) return;
  
  // Prefer to cut at the last sentence-ish boundary
  let cutIdx = -1;
  if (ttsBuffer.length >= MIN_CHUNK) {
    let m, last = -1;
    PUNCT_PATTERN.lastIndex = 0; // Reset regex state
    while ((m = PUNCT_PATTERN.exec(ttsBuffer)) !== null) {
      last = m.index + m[0].length;
    }
    if (last > 0) cutIdx = last;
    if (cutIdx < 0 && (force || ttsBuffer.length >= MAX_CHUNK)) {
      cutIdx = Math.min(ttsBuffer.length, MAX_CHUNK);
    }
  }
  
  if (cutIdx > 0) {
    const chunk = ttsBuffer.slice(0, cutIdx).trim();
    ttsBuffer = ttsBuffer.slice(cutIdx);
    
    // backpressure: don't let the queue explode
    if (queued >= QUEUE_LIMIT) {
      console.log('TTS queue full, canceling and restarting');
      synth.cancel(); 
      queued = 0;
    }
    
    if (chunk) enqueue(chunk);
  }
}

function speakProgressive(newText) {
  if (!speakToggle.checked || !newText) {
    console.log('TTS skipped:', { speakToggle: speakToggle.checked, newText });
    return;
  }
  
  console.log('TTS progressive chunk:', newText);
  
  // Add new text to buffer
  ttsBuffer += newText;
  
  // Try to flush at natural boundaries as text accumulates
  maybeFlush(false);
}

function finalizeProgressiveTTS() {
  // Flush what's left in the buffer
  if (ttsBuffer.trim()) {
    console.log('TTS finalizing remaining text:', ttsBuffer);
    
    // Split any long tail into manageable bits
    while (ttsBuffer.length) {
      maybeFlush(true);
      if (ttsBuffer.length && ttsBuffer.length < MIN_CHUNK) {
        enqueue(ttsBuffer);
        ttsBuffer = "";
      }
    }
  }
}

function stopProgressiveTTS() {
  console.log('TTS stopping progressive stream');
  ttsBuffer = '';
  queued = 0;
  synth.cancel();
  currentProgressiveStream = null;
}
document.addEventListener('touchstart', () => { try { speechSynthesis.cancel(); } catch(_){} }, { once:true });
if ('speechSynthesis' in window) {
  loadVoicesIntoSelect();
  window.speechSynthesis.onvoiceschanged = loadVoicesIntoSelect;
  speakToggle.checked = storage.speak;
  rateSlider.value  = storage.rate;  rateVal.textContent  = storage.rate.toFixed(2);
  pitchSlider.value = storage.pitch; pitchVal.textContent = storage.pitch.toFixed(2);
}
voiceSelect.onchange = () => { storage.voiceURI = voiceSelect.value; };
speakToggle.onchange = () => { storage.speak = speakToggle.checked; };
rateSlider.oninput  = () => { rateVal.textContent  = parseFloat(rateSlider.value).toFixed(2); storage.rate  = parseFloat(rateSlider.value); };
pitchSlider.oninput = () => { pitchVal.textContent = parseFloat(pitchSlider.value).toFixed(2); storage.pitch = parseFloat(pitchSlider.value); };

/* ------------------------- Chat UI ------------------------- */
function renderAll(){
  chatEl.innerHTML = "";
  history.forEach((msg) => appendBubble(msg.role, msg.content));
  chatEl.scrollTop = chatEl.scrollHeight;
}
function appendBubble(role, text){
  const div = document.createElement('div');
  div.className = 'bubble ' + (role === 'user' ? 'me' : role === 'assistant' ? 'bot' : 'sys');
  div.textContent = text;
  if (role === 'assistant') {
    const b = document.createElement('button');
    b.className = 'speak-btn';
    b.title = 'Speak this response';
    b.textContent = 'üîä';
    b.onclick = (e) => { e.stopPropagation(); speak(text); };
    div.appendChild(b);
  }
  chatEl.appendChild(div);
  chatEl.scrollTop = chatEl.scrollHeight;
}

/* ------------------------- Networking ------------------------- */
async function sendToJetson(userText){
  const server = srvEl.value.replace(/\/+$/, '');
  const model  = modelSelect.value.trim() || null;
  const payload = { messages: history, model };
  payload.messages.push({ role: "user", content: userText });

  if (streamToggle.checked) {
    return await sendToJetsonStream(userText, server, payload);
  } else {
    return await sendToJetsonRegular(server, payload);
  }
}

async function sendToJetsonRegular(server, payload) {
  const url = server + "/chat";
  const res = await fetch(url, {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify(payload)
  });
  if(!res.ok) throw new Error(await res.text());
  const data = await res.json();
  const reply = data.reply || "";
  return reply;
}

async function sendToJetsonStream(userText, server, payload) {
  const url = server + "/chat/stream";
  
  return new Promise((resolve, reject) => {
    // Reset TTS state for new response
    stopProgressiveTTS();
    
    // Since EventSource doesn't support POST, we'll use fetch with streaming
    fetch(url, {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify(payload)
    }).then(response => {
      if (!response.ok) {
        throw new Error(`HTTP ${response.status}: ${response.statusText}`);
      }
      
      const reader = response.body.getReader();
      const decoder = new TextDecoder();
      let fullReply = "";
      let currentBubble = null;
      
      function readStream() {
        reader.read().then(({ done, value }) => {
          if (done) {
            // Add final message to history
            history.push({ role:"user", content:userText });
            history.push({ role:"assistant", content:fullReply });
            localStorage.setItem('history', JSON.stringify(history));
            resolve(fullReply);
            return;
          }
          
          const chunk = decoder.decode(value, { stream: true });
          const lines = chunk.split('\n');
          
          for (const line of lines) {
            if (line.startsWith('data: ')) {
              try {
                const data = JSON.parse(line.slice(6));
                
                if (data.type === 'content') {
                  const content = data.content;
                  console.log('Stream content received:', content);
                  fullReply += content;
                  
                  // Create or update streaming bubble
                  if (!currentBubble) {
                    currentBubble = createStreamingBubble();
                  }
                  updateStreamingBubble(currentBubble, fullReply);
                  
                  // Speak the new content progressively
                  speakProgressive(content);
                  
                } else if (data.type === 'done') {
                  // Streaming complete - finalize any remaining text
                  finalizeProgressiveTTS();
                  
                  if (currentBubble) {
                    finalizeStreamingBubble(currentBubble);
                  }
                  
                } else if (data.type === 'error') {
                  reject(new Error(data.error));
                  return;
                }
              } catch (e) {
                // Skip malformed JSON
                continue;
              }
            }
          }
          
          readStream();
        }).catch(reject);
      }
      
      readStream();
    }).catch(reject);
  });
}

function createStreamingBubble() {
  const div = document.createElement('div');
  div.className = 'bubble bot streaming';
  div.textContent = '';
  chatEl.appendChild(div);
  chatEl.scrollTop = chatEl.scrollHeight;
  return div;
}

function updateStreamingBubble(bubble, content) {
  bubble.textContent = content;
  chatEl.scrollTop = chatEl.scrollHeight;
}

function finalizeStreamingBubble(bubble) {
  bubble.classList.remove('streaming');
  
  // Add speak button
  const b = document.createElement('button');
  b.className = 'speak-btn';
  b.title = 'Speak this response';
  b.textContent = 'üîä';
  b.onclick = (e) => { e.stopPropagation(); speak(bubble.textContent); };
  bubble.appendChild(b);
}
sendBtn.onclick = async () => {
  const text = txtEl.value.trim();
  if(!text) return;
  
  // Stop any ongoing TTS when user submits new message
  stopSpeaking();
  stopProgressiveTTS();
  
  txtEl.value = "";
  appendBubble('user', text);
  
  try {
    if (streamToggle.checked) {
      // For streaming, the bubble is created and updated in sendToJetsonStream
      await sendToJetson(text);
    } else {
      // For non-streaming, use the old approach
      const reply = await sendToJetson(text);
      appendBubble('assistant', reply);
      speak(reply);
    }
  } catch(err){
    appendBubble('system', 'Error: ' + err.message);
  }
};
txtEl.addEventListener('keydown', (e) => { 
  if(e.key === 'Enter') {
    // Stop TTS when user presses Enter to send message
    stopSpeaking();
    stopProgressiveTTS();
    sendBtn.click(); 
  }
});

// Stop TTS when user starts typing
txtEl.addEventListener('input', () => {
  stopSpeaking();
  stopProgressiveTTS();
});
saveBtn.onclick = () => {
  storage.server = srvEl.value;
  storage.model = modelSelect.value;
  appendBubble('system', 'Saved server/model.');
};

savePromptBtn.onclick = () => {
  const newPrompt = systemPromptEl.value.trim();
  if (!newPrompt) {
    appendBubble('system', 'System prompt cannot be empty.');
    return;
  }
  
  storage.systemPrompt = newPrompt;
  
  // Update the system message in history
  if (history.length > 0 && history[0].role === 'system') {
    history[0].content = newPrompt;
  } else {
    history.unshift({ role: 'system', content: newPrompt });
  }
  
  // Save updated history
  storage.history = history;
  
  appendBubble('system', 'System prompt saved and updated.');
};

/* ------------------------- WebAudio VAD + Recorder ------------------------- */
/* Simple energy-based VAD:
   - Measure RMS over frames from an AnalyserNode
   - Start recording when dB > START_DB for START_FRAMES
   - Stop when dB < STOP_DB for STOP_FRAMES (hangover)
   Tune thresholds below for your environment.
*/
const VAD_CFG = {
  pollMs: 40,          // analysis hop
  startDb: -45,        // dBFS above which we consider "speech started"
  stopDb:  -52,        // dBFS below which we consider "silence"
  startFrames: 3,      // need N consecutive frames above startDb
  stopFrames:  15,     // need N consecutive frames below stopDb (~600ms at 40ms hop)
  minSpeakMs: 400,     // ignore too-short bursts
};

let ac = null;
let micStream = null;
let analyser = null;
let floatBuf = null;
let meterTimer = null;

let recorder = null;
let recChunks = [];
let recMime = '';

let listening = false;
let speaking = false;
let speechFrames = 0;
let silenceFrames = 0;
let recordingActive = false;
let utterStartTs = 0;

function dbfs_of_frame(buf) {
  let sum = 0;
  for (let i=0; i<buf.length; i++) {
    const s = buf[i];
    sum += s*s;
  }
  const rms = Math.sqrt(sum / buf.length + 1e-12);
  const db = 20 * Math.log10(rms);
  return db; // [-Infinity, 0]
}

async function initVAD() {
  if (!ac) ac = new (window.AudioContext || window.webkitAudioContext)();
  if (!micStream) micStream = await navigator.mediaDevices.getUserMedia({ audio: { echoCancellation:true, noiseSuppression:true, autoGainControl:true } });
  const src = ac.createMediaStreamSource(micStream);
  analyser = ac.createAnalyser();
  analyser.fftSize = 1024;
  analyser.smoothingTimeConstant = 0.05;
  floatBuf = new Float32Array(analyser.fftSize);
  src.connect(analyser);

  // Prepare MediaRecorder for the same mic stream
  recMime = pickMimeType();
  recorder = new MediaRecorder(micStream, recMime ? { mimeType: recMime } : {});
  recorder.ondataavailable = (e) => { if (e.data && e.data.size > 0) recChunks.push(e.data); };
  recorder.onstop = onRecordingStop;

  hintEl.textContent = 'Listening‚Ä¶ start speaking to begin.';
  setVADState('Listening', true);

  listening = true;
  speechFrames = 0;
  silenceFrames = 0;
  meterTimer = setInterval(pollVAD, VAD_CFG.pollMs);
}

function setVADState(label, on) {
  vadStatePill.textContent = label;
  vadStatePill.classList.toggle('on', on);
  vadStatePill.classList.toggle('off', !on);
  toggleVADBtn.classList.toggle('on', on);
  toggleVADBtn.classList.toggle('off', !on);
}

function pickMimeType() {
  const cands = ['audio/webm;codecs=opus','audio/ogg;codecs=opus','audio/webm','audio/ogg','audio/wav'];
  for (const c of cands) if (MediaRecorder.isTypeSupported(c)) return c;
  return '';
}

function startRecordingNow() {
  if (recordingActive) return;
  
  // Stop any ongoing TTS when user starts speaking
  stopSpeaking();
  stopProgressiveTTS();
  
  recChunks = [];
  recorder.start();
  utterStartTs = performance.now();
  recordingActive = true;
  setVADState('Recording‚Ä¶', true);
  hintEl.textContent = 'Recording‚Ä¶ pause to finish.';
}

function stopRecordingNow() {
  if (!recordingActive) return;
  recordingActive = false;
  try { recorder.stop(); } catch {}
  setVADState('Processing‚Ä¶', true);
  hintEl.textContent = 'Transcribing‚Ä¶';
}

async function onRecordingStop() {
  // Send to /stt (Whisper)
  try {
    const blob = new Blob(recChunks, { type: recMime || recChunks[0]?.type || 'audio/webm' });
    const fd = new FormData();
    fd.append('file', blob, 'speech.webm');
    const server = srvEl.value.replace(/\/+$/, '');
    const res = await fetch(server + '/stt', { method:'POST', body: fd });
    if (!res.ok) throw new Error(await res.text());
    const { text } = await res.json();
    if (text && text.trim().length) {
      appendBubble('user', text.trim());
      const reply = await sendToJetson(text.trim());
      appendBubble('assistant', reply);
      speak(reply);
    } else {
      appendBubble('system', 'Silence or unrecognized speech.');
    }
  } catch (e) {
    appendBubble('system', 'STT error: ' + e.message);
  } finally {
    // Return to listening
    setVADState('Listening', true);
    hintEl.textContent = 'Listening‚Ä¶';
  }
}

function pollVAD() {
  if (!analyser || !floatBuf) return;
  analyser.getFloatTimeDomainData(floatBuf);
  const db = dbfs_of_frame(floatBuf);

  // Check if TTS is currently speaking
  speaking = speechSynthesis.speaking;

  // State machine:
  if (!recordingActive) {
    // WAITING FOR SPEECH
    if (db > VAD_CFG.startDb) {
      speechFrames++;
      if (speechFrames >= VAD_CFG.startFrames) {
        // Stop TTS immediately when speech is detected
        if (speaking) {
          stopSpeaking();
          stopProgressiveTTS();
        }
        startRecordingNow();
        speechFrames = 0;
        silenceFrames = 0;
      }
    } else {
      speechFrames = 0;
    }
  } else {
    // RECORDING; WAIT FOR SILENCE
    if (db < VAD_CFG.stopDb) {
      silenceFrames++;
      const durMs = performance.now() - utterStartTs;
      if (silenceFrames >= VAD_CFG.stopFrames && durMs >= VAD_CFG.minSpeakMs) {
        stopRecordingNow();
        silenceFrames = 0;
        speechFrames = 0;
      }
    } else {
      silenceFrames = 0;
    }
  }
}

/* ------------------------- Controls ------------------------- */
toggleVADBtn.onclick = async () => {
  if (!listening) {
    toggleVADBtn.disabled = true;
    try {
      await initVAD(); // prompts for mic permission (user gesture)
      toggleVADBtn.textContent = 'üõë Stop VAD';
      setVADState('Listening', true);
    } catch (e) {
      appendBubble('system', 'Mic error: ' + e.message);
      setVADState('Idle', false);
    } finally {
      toggleVADBtn.disabled = false;
    }
  } else {
    // stop listening
    listening = false;
    setVADState('Idle', false);
    toggleVADBtn.textContent = 'üé§ Start VAD';
    hintEl.textContent = 'Status: not listening.';
    try { clearInterval(meterTimer); } catch {}
    try { if (recorder && recorder.state === 'recording') recorder.stop(); } catch {}
    try { if (ac && ac.state !== 'closed') ac.close(); } catch {}
    ac = null; analyser = null; micStream = null; recorder = null;
  }
};
</script>
</body>
</html>